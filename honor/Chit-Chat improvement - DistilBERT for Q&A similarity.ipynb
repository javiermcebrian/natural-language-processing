{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chit-Chat improvement - DistilBERT for Q&A similarity\n",
    "\n",
    "In this honor project I propose to improve the dialogue model by means of an approach based on the suggestion 4 in the coursera assignment: *Selective model: embeddings-based ranking*.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The system will rank a list of answers from Cornell dataset, which are binded to questions that we have encoded previously using DistilBERT. At chatting time, the system perform question2embedding computation for the actual user's question using an endpoint deployed in a different EC2 machine. This endpoint is exposed by a Flask App inside a Docker that serves DistilBERT (here you can find my Dockerhub https://hub.docker.com/repository/docker/javiermcebrian/sentence_transformers_service).\n",
    "\n",
    "The reason for this split into services, is because loading DistilBERT into RAM memory surpases the resources provided by Free Tier in AWS. Once we have the served embedding that represents that user's question, we perform distance similarity to the staticaly loaded (pre-computed) questions' embeddings inside a Docker service deployed in the main EC2 machine (here you can find my Dockerhub https://hub.docker.com/repository/docker/javiermcebrian/coursera_nlp_honors_serve). With this approach we aim to improve chat quality as DistilBERT are better embeddings that the used during the course.\n",
    "\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "I wanted to try state-of-the-art models and to build custom applications using Docker. Appart from this notebook, I've required a lot of source code, docker containers, docker compose, bash scripts, etc. that I've develop. Here you can find all the related stuff: https://github.com/javiermcebrian/natural-language-processing/tree/master/honor:\n",
    "* bert_as_a_service (not used at the end): Dockerfile to launch a service for bert-as-a-service implementation (https://github.com/hanxiao/bert-as-service). I wanted to try different ways of computing embeddings, but finally I preferred other implementations.\n",
    "* datasets: as in the original coursera's repository\n",
    "* environment (for experimentation):\n",
    "    * environment-manager.sh: this is the entry point for running experiments. You can deploy experiment environment for both PyCharm interpreter and remote Notebook service. It provides a help guide if you use run it without args. It basically manages a docker compose yaml that offers this 2 services.\n",
    "    * docker-compose.yml: define the services for experimentation.\n",
    "    * Dockerfile.serve: base Dockerfile that is going to be useful for both EC2 cheap environment for the bot, and base docker for the experiment one.\n",
    "    * Dockerfile.experiments: it adds a lot of useful libraries for experimentation as well as jupyter cappabilities to serve notebook endpoints to connect at. It is bigger expensive (RAM and size) than serve one.\n",
    "    * requirements-XXXXXXXXXXX.txt: requirement files.\n",
    "* sentence_transformers_service (to serve DistilBERT in AWS EC2 instance):\n",
    "    * Dockerfile: defines service thath uses app.py Flask App\n",
    "    * app.py: service that manages DistilBERT. It assumes that the model is serialized in a predefined path (run docker using volumes). Provides singleton model management and warmup features (using PING) to get the model deployed before the user requires it.\n",
    "    * build_and_push.sh: build docker, tag it, and push to DockerHub\n",
    "    * start-service.sh: run the service in background\n",
    "* SRC: the rest of the code is the one provided by Coursera, updated with that functionality. It calls the DistilBERT service and perform the warmup via PING to the service. Finally it does the answering selection by embedding similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model\n",
    "First, we are going to test DistilBERT and to serialize the model for future usage in the serving app. We can compute embeddings of size 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "model.save('/root/coursera/artifacts/distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['Hi how are you']\n",
    "embeddings_dim = len(model.encode(sentences)[0])\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We load the Cornell dataset using max_len parameter equals to 100 to get large enough sentences. Then we perform random sampling as we have RAM constraints in the Free Tier AWS machines, and finally perform data cleaning using text_prepare() function. We achieve 34.109 possible responses from the dialogue model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import datasets\n",
    "from utils import text_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:04<00:00, 18387.97it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/root/coursera/datasets/data/cornell'\n",
    "data = datasets.readCornellData(dataset_path, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34109"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameters for sample a 20% of the QA data\n",
    "rate = 0.2\n",
    "indices = list(range(len(data)))\n",
    "nb = int(len(data) * rate)\n",
    "\n",
    "# sample the data and apply text_prepare() only to questions\n",
    "indices_selected = random.sample(indices, nb)\n",
    "data_selected = [(text_prepare(data[i][0]), data[i][1]) for i in indices_selected]\n",
    "\n",
    "# show the number of answers the system will be able to offer for chit-chat\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think likes think', 'finally came to your senses huh'),\n",
       " ('blankets notice warm fifty percent wool',\n",
       "  'they also smell of moth balls when were they issued this morning'),\n",
       " ('youre dark horse ripley engaged', 'your parents met her'),\n",
       " ('scattered smothered covered',\n",
       "  'exactly well i guess a couple more photos wont kill me'),\n",
       " ('youre anya rosson arent ive heard back new york',\n",
       "  'sorry i cant return the compliment'),\n",
       " ('jimmy', 'im not sure'),\n",
       " ('think', 'well thank goodness thats settled'),\n",
       " ('dont call doll larry hate call doll',\n",
       "  'you used to love it when i called you doll'),\n",
       " ('come cuervo delivered didnt im asking promised', 'well see'),\n",
       " ('oh love song isnt great doesnt make want dance cmon',\n",
       "  'uh well thats okay i dont dance heh heh')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selected[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distil-BERT Embeddings\n",
    "\n",
    "Here we pre-compute Distil-BERT sentence embeddings as for StackOverflow answers.\n",
    "\n",
    "#### Resources' constraints\n",
    "\n",
    "Each StackOverflow pickle has a size in mean of 100Mb 280.000 samples. Here, for Cornell dataset we have 34.109 samples. These are going to be saved into a pickle with larger embeddings than StackOverflow ones. This means that in size (RAM constraints) and in computational time (min distance between question embedding and these) they are going to compensate: less samples but larger vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = zip(*data_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [07:09<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "batchsize = 200\n",
    "\n",
    "q_embeddings = np.zeros((nb, embeddings_dim), dtype=np.float32)\n",
    "for i in tqdm(range(0, nb, batchsize)):\n",
    "    end = min(nb, i+batchsize)\n",
    "    q_embeddings[i:end, :] = model.encode(questions[i:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/coursera/artifacts/qa_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump((answers, q_embeddings), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting resources into RAM\n",
    "\n",
    "Here we would like to show that, theoretically, the proposed system fits into the constrained resources provided by Free Tier in AWS, using only 1 EC2 instance. However, in practice it's not true, so we have build the aforementioned service for DistilBERT in a different machine.\n",
    "\n",
    "Anyway, we describe the hypothesis: first we must restart the kernel inside the experiments Docker, then we are going to sequentially execute the following cells, to check the RAM increments. The followind command will give us this information:\n",
    "\n",
    "```sh\n",
    "docker ps -q | xargs  docker stats --no-stream\n",
    "```\n",
    "\n",
    "Here are the RAM usage, incrementally:\n",
    "- Docker experiments: 82.34Mb\n",
    "- Docker experiments + sentence_transformers: 155Mb\n",
    "- Docker experiments + sentence_transformers + distilbert: 582.5Mb\n",
    "- Docker experiments + sentence_transformers + distilbert + embeddings: 684.7Mb\n",
    "\n",
    "Non-chit-chat RAM resources (the rest of the resources, checked for project 1 chatbot at AWS) are of ~300Mb for the worst case, i.e., when the user request for a SatckOverflow answer, so the embeddings are loaded into memory.\n",
    "\n",
    "Anyway, docker for serving will be much lighter: 4.75Mb.\n",
    "\n",
    "With this we can estimate RAM usage in AWS: 684.7 - 82.34 + 4.75 + 300 = **907.11Mb for the worst case**, being the improbable time slot at which the service is releasing chit-chat model memory and reading StakOverflow embeddings into memory. This assumption is for a single user. It's not in the requirements to support multi-user as AWS Free Tier has few resources.\n",
    "\n",
    "As I've already mentioned, this does not work in practice, leading us to a server client solution for the DistilBERT stuff, but it's useful to roughly understand the RAM requirements that we have, to design our model. With this information we can decide if using DistilBERT or another different model, as well as how much question embeddings to store to perform similarity matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('/root/coursera/artifacts/distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('/root/coursera/artifacts/qa_embeddings.pkl', 'rb') as f:\n",
    "    qa_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments for predictions\n",
    "\n",
    "Here we provide some experiments using the model and the pre-computed embeddings to show how is the prediction function and how it works for some samples. The source code is at my Github repository (https://github.com/javiermcebrian/natural-language-processing/blob/master/honor/dialogue_manager.py and https://github.com/javiermcebrian/natural-language-processing/blob/master/honor/sentence_transformers_service/app.py) as I've already mentioned, with the prediction function, but additionaly I provide the experiments expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "\n",
    "def test_system(qa_embeddings, user_questions):\n",
    "    answers, q_embeddings = qa_embeddings\n",
    "    res = []\n",
    "    for uq in user_questions:\n",
    "        question_vec = np.expand_dims(model.encode([uq])[0], axis=0)\n",
    "        best_answer_id = pairwise_distances_argmin(X=question_vec, Y=q_embeddings, axis=1)[0]\n",
    "        res.append(answers[best_answer_id])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the park',\n",
       " 'its a huge moment in their life',\n",
       " 'it reminded me of you so i bought it it cost me more than all the others',\n",
       " 'thats what were paid for']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_questions = [\n",
    "    'Would you like to go to the park or to the office?',\n",
    "    'I would like to see your improvements',\n",
    "    'Why do you think they are beautiful?',\n",
    "    'Thank you for helping me'\n",
    "]\n",
    "\n",
    "test_system(qa_embeddings, user_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The results are quite good as they seem to be related to the fictitious conversation. We are asking the system about preferences (park or offic), self attributes (improvements), object's attributes (beautiful) and acknowledgements. Anyway, as any generative model, will be prone to significant errors from a human perspective as it's a complicated task. As a conclusion, I think the proposed system scales in AWS and provides interesting chit-chat features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
