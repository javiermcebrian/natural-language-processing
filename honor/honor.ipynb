{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "model.save('/root/coursera/artifacts/distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['Hi how are you']\n",
    "embeddings_dim = len(model.encode(sentences)[0])\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import datasets\n",
    "from utils import text_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:04<00:00, 18387.97it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/root/coursera/datasets/data/cornell'\n",
    "data = datasets.readCornellData(dataset_path, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34109"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameters for sample a 20% of the QA data\n",
    "rate = 0.2\n",
    "indices = list(range(len(data)))\n",
    "nb = int(len(data) * rate)\n",
    "\n",
    "# sample the data and apply text_prepare() only to questions\n",
    "indices_selected = random.sample(indices, nb)\n",
    "data_selected = [(text_prepare(data[i][0]), data[i][1]) for i in indices_selected]\n",
    "\n",
    "# show the number of answers the system will be able to offer for chit-chat\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think likes think', 'finally came to your senses huh'),\n",
       " ('blankets notice warm fifty percent wool',\n",
       "  'they also smell of moth balls when were they issued this morning'),\n",
       " ('youre dark horse ripley engaged', 'your parents met her'),\n",
       " ('scattered smothered covered',\n",
       "  'exactly well i guess a couple more photos wont kill me'),\n",
       " ('youre anya rosson arent ive heard back new york',\n",
       "  'sorry i cant return the compliment'),\n",
       " ('jimmy', 'im not sure'),\n",
       " ('think', 'well thank goodness thats settled'),\n",
       " ('dont call doll larry hate call doll',\n",
       "  'you used to love it when i called you doll'),\n",
       " ('come cuervo delivered didnt im asking promised', 'well see'),\n",
       " ('oh love song isnt great doesnt make want dance cmon',\n",
       "  'uh well thats okay i dont dance heh heh')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selected[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distil-BERT Embeddings\n",
    "\n",
    "Here we pre-compute compute Distil-BERT sentence embeddings as for StackOverflow answers.\n",
    "\n",
    "### Resources' constraints\n",
    "\n",
    "Each StackOverflow pickle has a size in mean of 100Mb 280.000 samples. Here, for Cornell dataset we have 34.109 samples. These are going to be saved into a pickle with larger embeddings than StackOverflow ones. This means that in size (RAM constraints) and in computational time (min distance between question embedding and these) they are going to compensate: less samples but larger vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = zip(*data_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [07:09<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "batchsize = 200\n",
    "\n",
    "q_embeddings = np.zeros((nb, embeddings_dim), dtype=np.float32)\n",
    "for i in tqdm(range(0, nb, batchsize)):\n",
    "    end = min(nb, i+batchsize)\n",
    "    q_embeddings[i:end, :] = model.encode(questions[i:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/coursera/artifacts/qa_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump((answers, q_embeddings), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting resources into RAM\n",
    "\n",
    "Here we would like to show that the proposed system fits into the constrained resources provided by Free Tier in AWS. First we must restart the kernel inside the experiments Docker, then we are going to sequentially execute the following cells, to check the RAM increments. The followind command will give us this information:\n",
    "\n",
    "```sh\n",
    "docker ps -q | xargs  docker stats --no-stream\n",
    "```\n",
    "\n",
    "Here are the RAM usage, incrementally:\n",
    "- Docker experiments: 82.34Mb\n",
    "- Docker experiments + sentence_transformers: 155Mb\n",
    "- Docker experiments + sentence_transformers + distilbert: 582.5Mb\n",
    "- Docker experiments + sentence_transformers + distilbert + embeddings: 684.7Mb\n",
    "\n",
    "Non-chit-chat RAM resources (the rest of the resources, checked for project 1 chatbot at AWS) are of ~300Mb for the worst case, i.e., when the user request for a SatckOverflow answer, so the embeddings are loaded into memory.\n",
    "\n",
    "Anyway, docker for serving will be much lighter: 4.75Mb.\n",
    "Here is my DockerHub repository with the Docker for serving: https://hub.docker.com/repository/docker/javiermcebrian/coursera_nlp_honors_serve/general\n",
    "\n",
    "With this we can estimate RAM usage in AWS: 684.7 - 82.34 + 4.75 + 300 = **907.11Mb for the worst case**, being the improbable time slot at which the service is releasing chit-chat model memory and reading StakOverflow embeddings into memory. This assumption is for a single user. It's not in the requirements to support multi-user as AWS Free Tier has few resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('/root/coursera/artifacts/distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('/root/coursera/artifacts/qa_embeddings.pkl', 'rb') as f:\n",
    "    qa_embeddings = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
